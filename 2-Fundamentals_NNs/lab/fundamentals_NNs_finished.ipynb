{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "EPSILON = 1e-8 # small constant to avoid underflow or divide per 0\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I - Data loading and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This time, the data will correspond to greyscale images. <br> Two different datasets can be used here:\n",
    "- The MNIST dataset, small 8*8 images, corresponding to handwritten digits &rightarrow; 10 classes\n",
    "- The Fashion MNIST dataset, medium 28*28 images, corresponding to clothes pictures &rightarrow; 10 classes\n",
    "\n",
    "#### Starting with the simple MNIST is recommended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"MNIST\"\n",
    "# dataset = \"FASHION_MNIST\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset='MNIST'):\n",
    "    if dataset == 'MNIST':\n",
    "        digits = load_digits()\n",
    "        X, Y = np.asarray(digits['data'], dtype='float32'), np.asarray(digits['target'], dtype='int32')\n",
    "        return X, Y\n",
    "    elif dataset == 'FASHION_MNIST':\n",
    "        import tensorflow as tf\n",
    "        fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "        (X, Y), (_, _) = fashion_mnist.load_data()\n",
    "        X = X.reshape((X.shape[0], X.shape[1] * X.shape[2]))\n",
    "        X, Y = np.asarray(X, dtype='float32'), np.asarray(Y, dtype='int32')\n",
    "        return X, Y\n",
    "X, Y = load_data(dataset=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = len(np.unique(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 1797\n",
      "Input dimension: 64\n",
      "Number of classes: 10\n"
     ]
    }
   ],
   "source": [
    "print('Number of samples: {:d}'.format(X.shape[0]))\n",
    "print('Input dimension: {:d}'.format(X.shape[1]))  # images 8x8 or 28*28 actually\n",
    "print('Number of classes: {:d}'.format(n_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range max-min of greyscale pixel values: (16.0, 0.0)\n",
      "First image sample:\n",
      "[ 0.  0.  5. 13.  9.  1.  0.  0.  0.  0. 13. 15. 10. 15.  5.  0.  0.  3.\n",
      " 15.  2.  0. 11.  8.  0.  0.  4. 12.  0.  0.  8.  8.  0.  0.  5.  8.  0.\n",
      "  0.  9.  8.  0.  0.  4. 11.  0.  1. 12.  7.  0.  0.  2. 14.  5. 10. 12.\n",
      "  0.  0.  0.  0.  6. 13. 10.  0.  0.  0.]\n",
      "First image label: 0\n",
      "Input design matrix shape: (1797, 64)\n"
     ]
    }
   ],
   "source": [
    "print(\"Range max-min of greyscale pixel values: ({0:.1f}, {1:.1f})\".format(np.max(X), np.min(X)))\n",
    "print(\"First image sample:\\n{0}\".format(X[0]))\n",
    "print(\"First image label: {0}\".format(Y[0]))\n",
    "print(\"Input design matrix shape: {0}\".format(X.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does the data look like?\n",
    "Each image in the dataset consists of a 8 x 8 (or 28 x 28) matrix, of greyscale pixels. For the MNIST dataset, the values are between 0 and 16 where 0 represents white, 16 represents black and there are many shades of grey in-between. For the Fashion MNIST dataset, the values are between 0 and 255.<br>Each image is assigned a corresponding numerical label, so the image in ```X[i]``` has its corresponding label stored in ```Y[i]```.\n",
    "\n",
    "The next cells below demonstrate how to visualise the input data. Make sure you understand what's happening, particularly how the indices correspond to individual items in the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_data_sample(X, Y, nrows=2, ncols=2):\n",
    "    fig, ax = plt.subplots(nrows, ncols)\n",
    "    for row in ax:\n",
    "        for col in row:\n",
    "            index = random.randint(0, X.shape[0])\n",
    "            dim = np.sqrt(X.shape[1]).astype(int)\n",
    "            col.imshow(X[index].reshape((dim, dim)), cmap=plt.cm.gray_r)\n",
    "            col.set_title(\"image label: %d\" % Y[index])\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_data_sample(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II - Multiclass classification MLP with Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II a) - Problem definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/mlp_mnist.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task here will be to implement \"from scratch\" a Multilayer Perceptron for classification.\n",
    "\n",
    "We will define the formal categorical cross entropy loss as follows:\n",
    "$$\n",
    "l(\\mathbf{\\Theta}, \\mathbf{X}, \\mathbf{Y}) = - \\frac{1}{n} \\sum_{i=1}^n \\log \\mathbf{f}(\\mathbf{x}_i ; \\mathbf{\\Theta})^\\top y_i\n",
    "$$\n",
    "<center>with $y_i$ being the one-hot encoded true label for the sample $i$, and $\\Theta = (\\mathbf{W}^h; \\mathbf{b}^h; \\mathbf{W}^o; \\mathbf{b}^o)$</center>\n",
    "<center>In addition, $\\mathbf{f}(\\mathbf{x}) = softmax(\\mathbf{z^o}(\\mathbf{x})) = softmax(\\mathbf{W}^o\\mathbf{h}(\\mathbf{x}) + \\mathbf{b}^o)$</center>\n",
    "<center>and $\\mathbf{h}(\\mathbf{x}) = g(\\mathbf{z^h}(\\mathbf{x})) = g(\\mathbf{W}^h\\mathbf{x} + \\mathbf{b}^h)$, $g$ being the activation function and could be implemented with $sigmoid$ or $relu$</center>\n",
    "\n",
    "## Objectives:\n",
    "- Write the categorical cross entropy loss function\n",
    "- Write the activation functions with their associated gradient\n",
    "- Write the softmax function that is going to be used to output the predicted probabilities\n",
    "- Implement the forward pass through the neural network\n",
    "- Implement the backpropagation according to the used loss: progagate the gradients using the chain rule and return $(\\mathbf{\\nabla_{W^h}}l ; \\mathbf{\\nabla_{b^h}}l ; \\mathbf{\\nabla_{W^o}}l ; \\mathbf{\\nabla_{b^o}}l)$\n",
    "- Implement dropout regularization in the forward pass: be careful to consider both training and prediction cases\n",
    "- Implement the SGD optimization algorithm, and improve it with simple momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple graph function to let you have a global overview:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/function_graph.png\" style=\"width: 750px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) You may find numpy outer products useful: <br>\n",
    "https://docs.scipy.org/doc/numpy/reference/generated/numpy.outer.html <br>\n",
    "We have: $outer(u, v) = u \\cdot v^T$, with $u, v$ two vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "u, v = np.random.normal(size=(5,)), np.random.normal(size=(10,))\n",
    "assert np.array_equal(\n",
    "    np.outer(u, v),\n",
    "    np.dot(np.reshape(u, (u.size, 1)), np.reshape(v, (1, v.size)))\n",
    ")\n",
    "assert np.outer(u, v).shape == (5, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) You also may find numpy matmul function useful: <br>\n",
    "https://docs.scipy.org/doc/numpy/reference/generated/numpy.matmul.html <br>\n",
    "It can be used to perform matrix products along one fixed dimension (i.e. the batch size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "A, B = np.random.randint(0, 100, size=(64, 5, 10)), np.random.randint(0, 100, size=(64, 10, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.array_equal(\n",
    "    np.stack([np.dot(A_i, B_i) for A_i, B_i in zip(A, B)]),\n",
    "    np.matmul(A, B)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II b) - Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron():\n",
    "    \"\"\"MLP with one hidden layer having a hidden activation,\n",
    "    and one output layer having a softmax activation\"\"\"\n",
    "    def __init__(self, X, Y, hidden_size, activation='relu',\n",
    "                 initialization='uniform', dropout=False, dropout_rate=1):\n",
    "        # input, hidden, and output dimensions on the MLP based on X, Y\n",
    "        self.input_size, self.output_size = X.shape[1], len(np.unique(Y))\n",
    "        self.hidden_size = hidden_size\n",
    "        # initialization strategies: avoid a full-0 initialization of the weight matrices\n",
    "        if initialization == 'uniform':\n",
    "            self.W_h = np.random.uniform(size=(self.hidden_size, self.input_size), high=0.01, low=-0.01)\n",
    "            self.W_o = np.random.uniform(size=(self.output_size, self.hidden_size), high=0.01, low=-0.01)\n",
    "        elif initialization == 'normal':\n",
    "            self.W_h = np.random.normal(size=(self.hidden_size, self.input_size), loc=0, scale=0.01)\n",
    "            self.W_o = np.random.normal(size=(self.output_size, self.hidden_size), loc=0, scale=0.01)\n",
    "        # the bias could be initializated to 0 or a random low constant\n",
    "        self.b_h = np.zeros(self.hidden_size)\n",
    "        self.b_o = np.zeros(self.output_size)\n",
    "        # our namedtuple structure of gradients\n",
    "        self.Grads = namedtuple('Grads', ['W_h', 'b_h', 'W_o', 'b_o'])\n",
    "        # and the velocities associated which are going to be useful for the momentum\n",
    "        self.velocities = {'W_h': 0., 'b_h': 0., 'W_o': 0., 'b_o': 0.}\n",
    "        # the hidden activation function used\n",
    "        self.activation = activation\n",
    "        # arrays to track back the losses and accuracies evolution\n",
    "        self.training_losses_history = []\n",
    "        self.validation_losses_history = []\n",
    "        self.training_acc_history = []\n",
    "        self.validation_acc_history = []\n",
    "        # train val split and normalization of the features\n",
    "        self.X_tr, self.X_val, self.Y_tr, self.Y_val = self.split_train_validation(X, Y)\n",
    "        self.scaler = MinMaxScaler(feature_range=(0, 1), copy=False)\n",
    "        self.X_tr = self.scaler.fit_transform(self.X_tr)\n",
    "        self.X_val = self.scaler.transform(self.X_val)\n",
    "        # dropout parameters\n",
    "        self.dropout = dropout\n",
    "        self.dropout_rate = dropout_rate\n",
    "        # step used for the optimization algorithm and setted later\n",
    "        self.step = None\n",
    "    \n",
    "    # One-hot encoding of the target\n",
    "    # Transform the integer represensation to a sparse one\n",
    "    @staticmethod\n",
    "    def one_hot(n_classes, Y):\n",
    "        return np.eye(n_classes)[Y]\n",
    "    \n",
    "    # Reverse one-hot encoding of the target\n",
    "    # Recover the former integer representation\n",
    "    # ex: from (0,0,1,0) to 2\n",
    "    @staticmethod\n",
    "    def reverse_one_hot(Y_one_hot):\n",
    "        return np.asarray(np.where(Y_one_hot==1)[1], dtype='int32')\n",
    "    \n",
    "    \"\"\"\n",
    "    Activation functions and their gradient\n",
    "    \"\"\"\n",
    "    # In implementations below X is a matrix of shape (n_samples, p)\n",
    "    \n",
    "    # A max_value value is indicated for the relu and grad_relu functions\n",
    "    # Make sure to clip the output to it to prevent numerical overflow (exploding gradient)\n",
    "    # Make it so the max value reachable is max_value\n",
    "    @staticmethod\n",
    "    def relu(X, max_value=20):\n",
    "        assert max_value > 0\n",
    "        # TODO:\n",
    "        return np.minimum(np.maximum(X,0), max_value)\n",
    "    \n",
    "    # Make it so the gradient becomes 0 when X becomes greater than max_value\n",
    "    @staticmethod\n",
    "    def grad_relu(X, max_value=20):\n",
    "        assert max_value > 0\n",
    "        # TODO:\n",
    "        if X<0 or X>max_value:\n",
    "            return np.zeros(X.shape)\n",
    "        else: \n",
    "            return X\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid(X):\n",
    "        # TODO:\n",
    "        return 1/(1+np.exp(-X))\n",
    "    \n",
    "    @staticmethod\n",
    "    def grad_sigmoid(X):\n",
    "        # TODO:\n",
    "        return MultiLayerPerceptron.sigmoid(X)/(1-MultiLayerPerceptron.sigmoid(X))\n",
    "    \n",
    "    # Softmax function to output probabilities\n",
    "    @staticmethod\n",
    "    def softmax(X):\n",
    "        # TODO:\n",
    "        return np.exp(X)/np.sum(np.exp(X), axis=1)\n",
    "    \n",
    "    # Loss function\n",
    "    # Consider using EPSILON to prevent numerical issues (log(0) is undefined)\n",
    "    # Y_true and Y_pred are of shape (n_samples,n_classes)\n",
    "    @staticmethod\n",
    "    def categorical_cross_entropy(Y_true, Y_pred):\n",
    "        # TODO:\n",
    "        return -np.mean(np.log(Y_pred+EPSILON)*Y_true)\n",
    "    \n",
    "    @staticmethod\n",
    "    def split_train_validation(X, Y, test_size=0.25, seed=False):\n",
    "        random_state = 42 if seed else np.random.randint(1e3)\n",
    "        X_tr, X_val, Y_tr, Y_val = train_test_split(X, Y, test_size=test_size, random_state=random_state)\n",
    "        return X_tr, X_val, Y_tr, Y_val\n",
    "    \n",
    "    # Sample random batch in (X, Y) with a given batch_size for SGD\n",
    "    @staticmethod\n",
    "    def get_random_batch(X, Y, batch_size):\n",
    "        indexes = np.random.choice(X.shape[0], size=batch_size, replace=False)\n",
    "        return X[indexes], Y[indexes]\n",
    "        \n",
    "    # Forward pass: compute f(x) as y, and return optionally the hidden states h(x) and z_h(x) for compute_grads\n",
    "    def forward(self, X, return_activation=False, training=False):\n",
    "        if self.activation == 'relu':\n",
    "            g_activation = self.relu\n",
    "        elif self.activation == 'sigmoid':\n",
    "            g_activation = self.sigmoid\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "            \n",
    "        z_h = np.dot(X, self.W_h) + self.bh\n",
    "        h = g_activation(z_h)\n",
    "        # TODO:\n",
    "        if self.dropout:\n",
    "            if training:\n",
    "                \n",
    "                h*=np.random.binomial(1, 1-self.dropout_rate, size=h.shape)\n",
    "                \n",
    "                pass\n",
    "            else:\n",
    "                # TODO:\n",
    "                h*=(1-self.dropout_rate)\n",
    "                pass\n",
    "            \n",
    "        # TODO:\n",
    "        y = g_activation(np.dot(h, self.W_o) + self.bo)\n",
    "        if return_activation:\n",
    "            return y, h, z_h\n",
    "        else:\n",
    "            return y\n",
    "    \n",
    "    # Backpropagation: return an instantiation of self.Grads that contains the average gradients for the given batch\n",
    "    def compute_grads(self, X, Y_true, vectorized=False):\n",
    "        if self.activation == 'relu':\n",
    "            g_grad = self.grad_relu\n",
    "        elif self.activation == 'sigmoid':\n",
    "            g_grad = self.grad_sigmoid\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        \n",
    "        if len(X.shape) == 1:\n",
    "            X = X.reshape((1,) + X.shape)\n",
    "        \n",
    "        if not vectorized:\n",
    "            n = X.shape[0]\n",
    "            grad_W_h = np.zeros((self.hidden_size, self.input_size))\n",
    "            grad_b_h = np.zeros((self.hidden_size, )) \n",
    "            grad_W_o = np.zeros((self.output_size, self.hidden_size))\n",
    "            grad_b_o = np.zeros((self.output_size, ))\n",
    "            for x, y_true in zip(X, Y_true):\n",
    "                y_pred, h, z_h = self.forward(x, return_activation=True, training=True)\n",
    "                \n",
    "                # TODO:\n",
    "                \n",
    "                \n",
    "            grads = self.Grads(grad_W_h/n, grad_b_h/n, grad_W_o/n, grad_b_o/n)\n",
    "            \n",
    "        else: \n",
    "            Y_pred, h, z_h = self.forward(X, return_activation=True, training=True)\n",
    "\n",
    "            # TODO (optional), try to do the backprop without Python loops in a vectorized way:\n",
    "            \n",
    "            grad_W_h = np.zeros((X.shape[0], self.hidden_size, self.input_size))\n",
    "            grad_b_h = np.zeros((X.shape[0], self.hidden_size, )) \n",
    "            grad_W_o = np.zeros((X.shape[0], self.output_size, self.hidden_size))\n",
    "            grad_b_o = np.zeros((X.shape[0], self.output_size, ))\n",
    "            \n",
    "            grads = self.Grads(\n",
    "                np.mean(grad_W_h, axis=0), \n",
    "                np.mean(grad_b_h, axis=0), \n",
    "                np.mean(grad_W_o, axis=0), \n",
    "                np.mean(grad_b_o, axis=0)\n",
    "            )\n",
    "            \n",
    "        return grads\n",
    "    \n",
    "    # Perform the update of the parameters (W_h, b_h, W_o, b_o) based of their gradient\n",
    "    def optimizer_step(self, optimizer='gd', momentum=False, momentum_alpha=0.9, \n",
    "                       batch_size=None, vectorized=True):\n",
    "        if optimizer == 'gd':\n",
    "            grads = self.compute_grads(self.X_tr, self.Y_tr, vectorized=vectorized)\n",
    "        elif optimizer == 'sgd':\n",
    "            batch_X_tr, batch_Y_tr = self.get_random_batch(self.X_tr, self.Y_tr, batch_size)\n",
    "            grads = self.compute_grads(batch_X_tr, batch_Y_tr, vectorized=vectorized)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        if not momentum:\n",
    "            # TODO:\n",
    "            pass\n",
    "        else:\n",
    "            # remember: use the stored velocities\n",
    "            # TODO:\n",
    "            pass\n",
    "    \n",
    "    # Loss wrapper\n",
    "    def loss(self, Y_true, Y_pred):\n",
    "        return self.categorical_cross_entropy(self.one_hot(self.output_size, Y_true), Y_pred)\n",
    "    \n",
    "    def loss_history_flush(self):\n",
    "        self.training_losses_history = []\n",
    "        self.validation_losses_history = []\n",
    "        \n",
    "    # Main function that trains the MLP with a design matrix X and a target vector Y\n",
    "    def train(self, optimizer='sgd', momentum=False, min_iterations=500, max_iterations=5000, initial_step=1e-1,\n",
    "              batch_size=64, early_stopping=True, early_stopping_lookbehind=100, early_stopping_delta=1e-4, \n",
    "              vectorized=False, flush_history=True, verbose=True):\n",
    "        if flush_history:\n",
    "            self.loss_history_flush()\n",
    "        cpt_patience, best_validation_loss = 0, np.inf\n",
    "        iteration_number = 0\n",
    "        self.step = initial_step\n",
    "        while len(self.training_losses_history) < max_iterations:\n",
    "            iteration_number += 1\n",
    "            self.optimizer_step(\n",
    "                optimizer=optimizer, momentum=momentum, batch_size=batch_size, vectorized=vectorized\n",
    "            )\n",
    "            \n",
    "            training_loss = self.loss(self.Y_tr, self.forward(self.X_tr))\n",
    "            self.training_losses_history.append(training_loss)\n",
    "            training_accuracy = self.accuracy_on_train()\n",
    "            self.training_acc_history.append(training_accuracy)\n",
    "            validation_loss = self.loss(self.Y_val, self.forward(self.X_val))\n",
    "            self.validation_losses_history.append(validation_loss)\n",
    "            validation_accuracy = self.accuracy_on_validation()\n",
    "            self.validation_acc_history.append(validation_accuracy)\n",
    "            if iteration_number > min_iterations and early_stopping:\n",
    "                if validation_loss + early_stopping_delta < best_validation_loss:\n",
    "                    best_validation_loss = validation_loss\n",
    "                    cpt_patience = 0\n",
    "                else:\n",
    "                    cpt_patience += 1\n",
    "            if verbose:\n",
    "                msg = \"iteration number: {0}\\t training loss: {1:.4f}\\t\" + \\\n",
    "                \"validation loss: {2:.4f}\\t validation accuracy: {3:.4f}\"\n",
    "                print(msg.format(iteration_number, \n",
    "                                 training_loss, \n",
    "                                 validation_loss,\n",
    "                                 validation_accuracy))\n",
    "            if cpt_patience >= early_stopping_lookbehind:\n",
    "                break\n",
    "    \n",
    "    # Return the predicted class once the MLP has been trained\n",
    "    def predict(self, X, normalize=True):\n",
    "        if normalize:\n",
    "            X = self.scaler.transform(X)\n",
    "        if len(X.shape) == 1:\n",
    "            return np.argmax(self.forward(X))\n",
    "        else:\n",
    "            return np.argmax(self.forward(X), axis=1)\n",
    "        \n",
    "    \"\"\"\n",
    "    Metrics and plots\n",
    "    \"\"\"\n",
    "    def accuracy_on_train(self):\n",
    "        return (self.predict(self.X_tr, normalize=False) == self.Y_tr).mean()\n",
    "\n",
    "    def accuracy_on_validation(self):\n",
    "        return (self.predict(self.X_val, normalize=False) == self.Y_val).mean()\n",
    "\n",
    "    def plot_loss_history(self, add_to_title=None):\n",
    "        import warnings\n",
    "        warnings.filterwarnings(\"ignore\")\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.plot(range(len(self.training_losses_history)), \n",
    "                 self.training_losses_history, label='Training loss evolution')\n",
    "        plt.plot(range(len(self.validation_losses_history)), \n",
    "                 self.validation_losses_history, label='Validation loss evolution')\n",
    "        plt.legend(fontsize=15)\n",
    "        plt.yscale('log')\n",
    "        plt.xlabel(\"iteration number\", fontsize=15)\n",
    "        plt.ylabel(\"Cross entropy loss\", fontsize=15)\n",
    "        base_title = \"Cross entropy loss evolution during training\"\n",
    "        if not self.dropout:\n",
    "            base_title += \", no dropout penalization\"\n",
    "        else:\n",
    "            base_title += \", {:.1f} dropout penalization\"\n",
    "            base_title = base_title.format(self.dropout_rate)\n",
    "        title = base_title + \", \" + add_to_title if add_to_title else base_title\n",
    "        plt.title(title, fontsize=20)\n",
    "        plt.show()\n",
    "        \n",
    "    def plot_validation_prediction(self, sample_id):\n",
    "        fig, (ax0, ax1) = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))\n",
    "        classes = np.unique(self.Y_tr)\n",
    "        dim = np.sqrt(self.X_val.shape[1]).astype(int)\n",
    "        ax0.imshow(self.scaler.inverse_transform([self.X_val[sample_id]]).reshape(dim, dim), cmap=plt.cm.gray_r,\n",
    "                   interpolation='nearest')\n",
    "        ax0.set_title(\"True image label: %d\" % self.Y_val[sample_id]);\n",
    "\n",
    "        ax1.bar(classes, self.one_hot(len(classes), self.Y_val[sample_id]), label='true')\n",
    "        ax1.bar(classes, self.forward(self.X_val[sample_id]), label='prediction', color=\"red\")\n",
    "        ax1.set_xticks(classes)\n",
    "        prediction = self.predict(self.X_val[sample_id], normalize=False)\n",
    "        ax1.set_title('Output probabilities (prediction: %d)' % prediction)\n",
    "        ax1.set_xlabel('Digit class')\n",
    "        ax1.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MultiLayerPerceptron(X, Y, hidden_size=50, activation='relu')\n",
    "mlp.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions:\n",
    "#### - Did you succeed to train the MLP and get a high validation accuracy? <br> Display available metrics (training and validation accuracies, training and validation losses)\n",
    "#### - Plot the prediction for a given validation sample. Is it accurate?\n",
    "#### - Compare the full gradient descent with the SGD.\n",
    "#### - Play with the hyper parameters you have: the hidden size, the activation function, the initial step and the batch size. <br> Comment. Don't hesitate to visualize results.\n",
    "#### - Once properly implemented, compare the training using early stopping, dropout, or both of them. <br> Why are these methods useful here?\n",
    "#### - Once properly implemented, compare the training using momentum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.plot_validation_prediction(sample_id=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.plot_loss_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III - Multiclass classification MLP with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Implement the same network architecture with Keras;\n",
    "    - First using the Sequential API\n",
    "    - Secondly using the functional API\n",
    "\n",
    "#### - Check that the Keras model can approximately reproduce the behavior of the Numpy model.\n",
    "\n",
    "#### - Compute the negative log likelihood of a sample 42 in the test set (can use `model.predict_proba`).\n",
    "\n",
    "#### - Compute the average negative log-likelihood on the full test set.\n",
    "\n",
    "#### - Compute the average negative log-likelihood  on the full training set and check that you can get the value of the loss reported by Keras.\n",
    "\n",
    "#### - Is the model overfitting or underfitting? (ensure that the model has fully converged by increasing the number of epochs to 500 or more if necessary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, X_val, Y_tr, Y_val = train_test_split(X, Y)\n",
    "scaler = MinMaxScaler(feature_range=(0, 1), copy=False)\n",
    "X_tr = scaler.fit_transform(X_tr)\n",
    "X_val = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = X[0].shape[0]\n",
    "n_classes = len(np.unique(Y_tr))\n",
    "n_hidden = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "activation = \"relu\"\n",
    "# activation = \"sigmoid\"\n",
    "\n",
    "print('Model with {} activation'.format(activation))\n",
    "keras_model = Sequential()\n",
    "# TODO:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "activation = \"relu\"\n",
    "# activation = \"sigmoid\"\n",
    "\n",
    "print('Model with {} activation'.format(activation))\n",
    "\n",
    "inputs = Input(shape=(n_features,))\n",
    "# TODO:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now that you know if the model is underfitting or overfitting:\n",
    "#### - In case of underfitting, can you explain why ? Also change the structure of the 2 previous networks to cancell underfitting\n",
    "#### - In case of overfitting, explain why and change the structure of the 2 previous networks to cancell the overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
