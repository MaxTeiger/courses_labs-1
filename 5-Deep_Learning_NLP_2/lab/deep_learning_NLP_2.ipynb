{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import operator\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "sys.path.append(\".\")\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an introduction to basic sequence-to-sequence learning using a Long short term memory (LSTM) module.\n",
    "Given a string of characters representing a math problem \"3141+42\" we would like to generate a string of characters representing the correct solution: \"3183\". Our network will learn how to do basic mathematical operations.\n",
    "The important part is that we will not first use our human intelligence to break the string up into integers and a mathematical operator. We want the computer to figure all that out by itself.\n",
    "Each math problem is an input sequence: a list of {0,...,9} integers and math operation symbols\n",
    "The result of the operation (\"$3141+42$\" $\\rightarrow$ \"$3183$\"</span>) is the sequence to decode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**math_operators** is the set of $5$ operations we are going to use to build are input sequences.<br/>\n",
    "The math_expressions_generation function uses them to generate a large set of examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def math_expressions_generation(n_samples=1000, n_digits=3, invert=True):\n",
    "    X, Y = [], []\n",
    "    math_operators = {\n",
    "        \"+\": operator.add,\n",
    "        \"-\": operator.sub,\n",
    "        \"*\": operator.mul,\n",
    "        \"/\": operator.truediv,\n",
    "        \"%\": operator.mod,\n",
    "    }\n",
    "    for i in range(n_samples):\n",
    "        a, b = np.random.randint(1, 10 ** n_digits, size=2)\n",
    "        op = np.random.choice(list(math_operators.keys()))\n",
    "        res = math_operators[op](a, b)\n",
    "        x = \"\".join([str(elem) for elem in (a, op, b)])\n",
    "        if invert is True:\n",
    "            x = x[::-1]\n",
    "        y = \"{:.5f}\".format(res) if isinstance(res, float) else str(res)\n",
    "        X.append(x)\n",
    "        Y.append(y)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "377*236 = 88972\n",
      "178/347 = 0.51297\n",
      "480-903 = -423\n",
      "446/594 = 0.75084\n",
      "600/761 = 0.78844\n",
      "380*438 = 166440\n",
      "818+86 = 904\n",
      "369+836 = 1205\n",
      "379%966 = 379\n",
      "197/141 = 1.39716\n",
      "548+45 = 593\n",
      "581+847 = 1428\n",
      "886/218 = 4.06422\n",
      "753%134 = 83\n",
      "152%922 = 152\n",
      "955*557 = 531935\n",
      "952/439 = 2.16856\n",
      "151/334 = 0.45210\n",
      "274/91 = 3.01099\n",
      "559-990 = -431\n"
     ]
    }
   ],
   "source": [
    "quick_for_debugg = True\n",
    "n_samples = 100 if quick_for_debugg else int(1e5)\n",
    "\n",
    "X, y = math_expressions_generation(n_samples=n_samples, n_digits=3, invert=True)\n",
    "for X_i, y_i in list(zip(X, y))[:20]:\n",
    "    print(X_i[::-1], \"=\", y_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I - Encoder and decoder models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- encoder and decoder are both GRU models\n",
    "- encoder and decoder both take an input sequence and output $1$ hidden vector for each step in input sequence\n",
    "- the decoder also outputs $1$ softmax per step in input sequence, that corresponds to the next predicted token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cells the example is:\n",
    "- sequence to encode: 94+8\n",
    "- sequence to decode: $102\\text{<EOS>}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NB: In this TP all tensors have a $\\text{batch_size}$ axis in addition to the traditional $\\text{nb_timesteps, vector_dim}$ axes.**\n",
    "**The batch size axis is there because pytorch GRU (and most other pytorch layers) can process tensors organized in batch, meaning that contain several sequences.**\n",
    "**In the returned tensor, the results for each sequence are given along a batch axis.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**encoder and decoder inputs**\n",
    "- for the encoder, the input sequence is the operation: $94+8$\n",
    "<img src=\"../images/encoder_input.png\" style=\"width: 600px;\" />\n",
    "- for the decoder, if using teacher forcing, the input sequence is the off-set of the sequence to decode: $\\text{<GO>}102$\n",
    "<img src=\"../images/decoder_input_all.png\" style=\"width: 600px;\" />\n",
    "- for the decoder, if **not** using teacher forcing, the input sequence is $1$ timestep long and is either the $\\text{<GO>}$ token or the previous predicted token:\n",
    "<img src=\"../images/decoder_input_one.png\" style=\"width: 600px;\" />\n",
    "for the decoder those $3$ scenarios are one: the input sequence is of shape $(\\text{nb_timesteps, batch_size, input_dim})$, the decoder goes through all timesteps for each sequence, produces $1$ hidden vector and $1$ prediction per timestep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**no attention vs attention**\n",
    "the attention mechanism is handled (and implemented) at the decoder level\n",
    "**no attention**\n",
    "<img src=\"../images/decoder_no_attention_all.png\" style=\"width: 900px;\" />\n",
    "At each timestep, the hidden vector is used to predict the next token\n",
    "**attention**\n",
    "<img src=\"../images/decoder_attention_all.png\" style=\"width: 900px;\" />\n",
    "The attention mechanism here is of type that is performed over the decoder hidden vectors after they are produced.\n",
    "- For each timestep of the decoder input, similarity between the decoder hidden vector and all the encoder hidden vectors is computed. It allows to determine which token in encoder input to focus on. Here similarity is just a dot product $hdec^T \\cdot henc$ between the vectors.\n",
    "- For each timestep of the decoder input, pass this \"attention weights\" vector to a softmax so the weights sum to $1$.\n",
    "- For each timestep of the decoder input, compute a weighted sum of the encoder hidden vectors. This is the context vector. The fact that it is more or less heavily weighted towards certain encoder hidden vector relates to the tokens the algorithm focuses on.\n",
    "- Use the context vector to predict the next token by performing a matrix product to set at the right dimension and apply a softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, device):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.device = device\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gru = nn.GRU(input_size, hidden_size).to(self.device)\n",
    "\n",
    "    \"\"\"\n",
    "    Implement the encoder forward pass.\n",
    "    Compute henc_ts, a tensor that represent all the encoder hidden vectors\n",
    "    for all timesteps for all sequences\n",
    "    henc_ts is of shape (nb_timesteps, batch_size, hidden_size)\n",
    "    Compute henc_final, the final encoder hidden vector for all sequences. \n",
    "    henc_final is of shape (1, batch_size, hidden_size)\n",
    "    note:\n",
    "        - encoder_input is of shape (nb_timesteps, batch_size, input_size)\n",
    "    hints:\n",
    "        - Use the gru attribute\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, encoder_input, henc_init=None):\n",
    "        if henc_init is None:\n",
    "            henc_init = torch.zeros(\n",
    "                1, encoder_input.size()[1], self.hidden_size, device=self.device\n",
    "            ).to(self.device)\n",
    "        # TODO:\n",
    "        henc_ts, henc_final = self.gru(encoder_input, henc_init)\n",
    "        \n",
    "        \n",
    "        return henc_ts, henc_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rnn = nn.GRU(10, 20, 2)\n",
    "# input = torch.randn(5, 3, 10)\n",
    "# h0 = torch.randn(2, 3, 20)\n",
    "# output, hn = rnn(input, h0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, device, attention=False):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.device = device\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gru = nn.GRU(output_size, hidden_size).to(self.device)\n",
    "        self.linear = nn.Linear(hidden_size, output_size).to(self.device)\n",
    "        self.attention = attention\n",
    "\n",
    "    \"\"\"\n",
    "    Implement the decoder forward pass.\n",
    "    Compute hdec_ts, a tensor that represent all the decoder hidden vectors\n",
    "    for all timesteps for all sequences\n",
    "    hdec_ts is of shape (nb_timesteps, batch_size, hidden_size)\n",
    "    Compute h_ts, a tensor that represent all the encoder hidden vectors\n",
    "    for all timesteps for all sequences\n",
    "    Compute hdec_final, the final decoder hidden vector for all sequences.\n",
    "    hdec_final is of shape (1, batch_size, hidden_size)\n",
    "        Hint: Use the gru attribute \n",
    "    Compute output, the tensor that represent all the softmax for all timesteps \n",
    "    for all sequences\n",
    "    output is of shape (nb_timesteps, batch_size, hidden_size)\n",
    "        without attention\n",
    "        with attention\n",
    "            compute first context_vectors, a tensor that represent a weighted sum\n",
    "            of encoder hidden vectors at all timesteps for all sequences. \n",
    "            context_vectors is of shape (nb_timesteps, batch_size, hidden_size)\n",
    "                Hint: it is possible to compute it in fully \"vectorial\" way with \n",
    "                pytorch function but do not hesitate to use loops to iterate over\n",
    "                timesteps etc. if it seems easier\n",
    "    \n",
    "    note:\n",
    "        - for the softmax, use the function torch.nn.functional.log_softmax\n",
    "        - follow the above diagrams\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, decoder_input, hdec_init, henc_ts=None):\n",
    "\n",
    "        # TODO:\n",
    "        hdec_ts, hdec_final = self.gru(decoder_input, hdec_init)\n",
    "        \n",
    "        if self.attention:\n",
    "            # TODO: (done)\n",
    "            batch_size = hdec_ts.shape[1]\n",
    "            nb_timesteps = hdec_ts.shape[0]\n",
    "            context_vectors = []\n",
    "            for b in range(batch_size):\n",
    "                context = []\n",
    "                for t in range(nb_timesteps):\n",
    "                    # Compute similarity\n",
    "                    sim = hdec_ts[t, b, :].matmul(henc_ts[:, b, :].transpose(0, 1))\n",
    "                    # Apply softmax\n",
    "                    sm_sim = F.log_softmax(sim)\n",
    "                    # Compute context vector\n",
    "                    context.append(torch.sum(sm_sim * henc_ts[:, b, :].transpose(0, 1), 1).unsqueeze(0))\n",
    "                # Concatenate to get context tensor\n",
    "                context_vectors.append(torch.cat(context).unsqueeze(0))\n",
    "            # Concatenate to get context tensor for all batches\n",
    "            context_vectors = torch.cat(context_vectors, 0).permute(1, 0, 2)\n",
    "            \n",
    "            # Compute output\n",
    "            output = F.log_softmax(self.linear(context_vectors))\n",
    "            \n",
    "            else:\n",
    "            # TODO:\n",
    "            output = F.log_softmax(self.linear(hdec_ts))\n",
    "            \n",
    "        return output, hdec_final\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II - Sequence to sequence model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GO** is the character (\"=\") that marks the beginning of decoding for the decoder GRU<br/>\n",
    "**EOS** is the character (\"\\n\") that marks the end of sequence to decode for the decoder GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**global Seq2seq architecture (teacher forcing scenario)**\n",
    "<img src=\"../images/seq2seq_teacher.png\" style=\"width: 1000px;\" />\n",
    "the teacher forcing mechanism is handled (and implemented) at the seq2seq forward pass level.\n",
    "teacher forcing or no teacher forcing depends on the kind of input passed to the decoder.\n",
    "**teacher forcing**\n",
    "<img src=\"../images/teacher_forcing.png\" style=\"width: 600px;\" />\n",
    "- the decoder input is the sequence of expected decoded tokens at all timesteps.\n",
    "- the decoder input is passed in one go to the decoder. The decoder goes through all timesteps and decodes the whole sequence in one go.\n",
    "- the decoder input is of shape $(\\text{nb_timesteps, batch_size, input_dim})$.\n",
    "**no teacher forcing**\n",
    "<img src=\"../images/no_teacher_forcing.png\" style=\"width: 1000px;\" />\n",
    "- the decoder input is $1$ timestep long and either the $\\text{GO}$ token or the previous decoded token\n",
    "- the decoder inputs are passed iteratively in many stages to the decoder. For each stage, the decoder is given as state the previous returned hidden vector and take as input the previous decoded token. It produces a new hidden vector and decoded token that are returned for the next stage.\n",
    "- the decoder input for each stage is of shape $(\\text{1, batch_size, input_dim})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Seq2seq(nn.Module):\n",
    "    def __init__(self, X, y, hidden_size=256, learning_rate=0.01, attention=False):\n",
    "        super(Seq2seq, self).__init__()\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.GO = \"=\"\n",
    "        self.EOS = \"\\n\"\n",
    "        self.dataset_size = None\n",
    "        self.encoder_char_index = None\n",
    "        self.encoder_index_char = None\n",
    "        self.decoder_char_index = None\n",
    "        self.decoder_index_char = None\n",
    "        self.encoder_vocabulary_size = None\n",
    "        self.decoder_vocabulary_size = None\n",
    "        self.max_encoder_sequence_length = None\n",
    "        self.max_decoder_sequence_length = None\n",
    "        self.encoder_input_tr = None\n",
    "        self.encoder_input_val = None\n",
    "        self.decoder_input_tr = None\n",
    "        self.decoder_input_val = None\n",
    "        self.target_tr = None\n",
    "        self.target_val = None\n",
    "        self._set_data_properties_attributes()\n",
    "        self._construct_data_set()\n",
    "        self.encoder = EncoderRNN(\n",
    "            input_size=self.encoder_vocabulary_size,\n",
    "            hidden_size=hidden_size,\n",
    "            device=self.device,\n",
    "        )\n",
    "        self.decoder = DecoderRNN(\n",
    "            hidden_size=hidden_size,\n",
    "            output_size=self.decoder_vocabulary_size,\n",
    "            attention=attention,\n",
    "            device=self.device,\n",
    "        )\n",
    "        self.parameters = list(self.encoder.parameters()) + list(\n",
    "            self.decoder.parameters()\n",
    "        )\n",
    "        self.optimizer = optim.Adam(self.parameters, lr=learning_rate)\n",
    "        self.criterion = nn.NLLLoss(reduction=\"mean\")\n",
    "        # training attributes\n",
    "        self.total_loss = None\n",
    "        self.total_loss_nb_samples = None\n",
    "\n",
    "    def _set_data_properties_attributes(self):\n",
    "        self.y = list(map(lambda token: self.GO + token + self.EOS, self.y))\n",
    "        self.dataset_size = len(self.X)\n",
    "        encoder_characters = sorted(list(set(\"\".join(self.X))))\n",
    "        decoder_characters = sorted(list(set(\"\".join(self.y))))\n",
    "        decoder_characters.remove(self.EOS)\n",
    "        # set EOS at 0 index so argmax on zero vector falls at EOS\n",
    "        decoder_characters = [self.EOS] + decoder_characters\n",
    "        self.encoder_char_index = dict((c, i) for i, c in enumerate(encoder_characters))\n",
    "        self.encoder_index_char = dict((i, c) for i, c in enumerate(encoder_characters))\n",
    "        self.decoder_char_index = dict((c, i) for i, c in enumerate(decoder_characters))\n",
    "        self.decoder_index_char = dict((i, c) for i, c in enumerate(decoder_characters))\n",
    "        self.encoder_vocabulary_size = len(self.encoder_char_index)\n",
    "        self.decoder_vocabulary_size = len(self.decoder_char_index)\n",
    "        self.max_encoder_sequence_length = max([len(sequence) for sequence in self.X])\n",
    "        self.max_decoder_sequence_length = max([len(sequence) for sequence in self.y])\n",
    "        print(\"Number of samples:\", self.dataset_size)\n",
    "        print(\"Number of unique encoder tokens:\", self.encoder_vocabulary_size)\n",
    "        print(\"Number of unique decoder tokens:\", self.decoder_vocabulary_size)\n",
    "        print(\"Max sequence length for encoding:\", self.max_encoder_sequence_length)\n",
    "        print(\"Max sequence length for decoding:\", self.max_decoder_sequence_length)\n",
    "\n",
    "    def _construct_data_set(self):\n",
    "        encoder_input = torch.zeros(\n",
    "            (\n",
    "                self.max_encoder_sequence_length,\n",
    "                self.dataset_size,\n",
    "                self.encoder_vocabulary_size,\n",
    "            ),\n",
    "            dtype=torch.float32,\n",
    "        )\n",
    "        decoder_input = torch.zeros(\n",
    "            (\n",
    "                self.max_decoder_sequence_length,\n",
    "                self.dataset_size,\n",
    "                self.decoder_vocabulary_size,\n",
    "            ),\n",
    "            dtype=torch.float32,\n",
    "        )\n",
    "        target = torch.zeros(\n",
    "            (\n",
    "                self.max_decoder_sequence_length,\n",
    "                self.dataset_size,\n",
    "                self.decoder_vocabulary_size,\n",
    "            ),\n",
    "            dtype=torch.float32,\n",
    "        )\n",
    "\n",
    "        for i, (X_i, y_i) in enumerate(zip(self.X, self.y)):\n",
    "            for t, char in enumerate(X_i):\n",
    "                encoder_input[t, i, self.encoder_char_index[char]] = 1.0\n",
    "            for t, char in enumerate(y_i):\n",
    "                decoder_input[t, i, self.decoder_char_index[char]] = 1.0\n",
    "                if t > 0:\n",
    "                    target[t - 1, i, self.decoder_char_index[char]] = 1.0\n",
    "\n",
    "        p_val = 0.25\n",
    "        size_val = int(p_val * self.dataset_size)\n",
    "        idxs = np.arange(self.dataset_size)\n",
    "        np.random.shuffle(idxs)\n",
    "        idxs_tr = idxs[:-size_val]\n",
    "        idxs_val = idxs[-size_val:]\n",
    "        (\n",
    "            self.encoder_input_tr,\n",
    "            self.encoder_input_val,\n",
    "            self.decoder_input_tr,\n",
    "            self.decoder_input_val,\n",
    "            self.target_tr,\n",
    "            self.target_val,\n",
    "        ) = (\n",
    "            encoder_input[:, idxs_tr, :],\n",
    "            encoder_input[:, idxs_val, :],\n",
    "            decoder_input[:, idxs_tr, :],\n",
    "            decoder_input[:, idxs_val, :],\n",
    "            target[:, idxs_tr, :],\n",
    "            target[:, idxs_val, :],\n",
    "        )\n",
    "        self.encoder_input_tr = self.encoder_input_tr.to(self.device)\n",
    "        self.encoder_input_val = self.encoder_input_val.to(self.device)\n",
    "        self.decoder_input_tr = self.decoder_input_tr.to(self.device)\n",
    "        self.decoder_input_val = self.decoder_input_val.to(self.device)\n",
    "        self.target_tr = self.target_tr.to(self.device)\n",
    "        self.target_val = self.target_val.to(self.device)\n",
    "\n",
    "    \"\"\"\n",
    "    Implement the Seq2seq forward pass.\n",
    "    Compute henc_ts, the tensor that represent all the encoder hidden vectors\n",
    "    for all timesteps for all sequences\n",
    "    henc_ts is of shape (nb_encoder_timesteps, batch_size, hidden_size)\n",
    "    Compute henc_final, the final encoder hidden vector for all sequences. \n",
    "    henc_final is of shape (1, batch_size, hidden_size)\n",
    "    Compute pred_softmax_all_ts, the tensor that represents all the softmax\n",
    "    vectors at all timesteps for all sequences.\n",
    "    pred_softmax_all_ts is of shape (nb_decoder_timesteps, batch_size, output_dim)\n",
    "        teacher forcing case\n",
    "            Hint: refer to diagrams notes\n",
    "        no teacher forcing case\n",
    "            Before the loop, initialize decoder_input, the tensor that represents\n",
    "            the first token passed to the decoder for all sequences. \n",
    "            The token is <GO>, the decoder_input is of shape (1, batch_size, output_dim).\n",
    "            It has to be in one-hot encoding representation.\n",
    "            In the loop, compute pred_softmax. The tensor represents the softmax \n",
    "            produced at this timestep, for all sequences. \n",
    "            It is of shape (1, batch_size, output_dim)\n",
    "            In the loop, compute hdec_final. The tensor represents the hidden vector \n",
    "            produced at this timestep, for all sequences. \n",
    "            It is of shape (1, batch_size, hidden_dim)\n",
    "            In the loop, set hdec_init to the right value. \n",
    "            hdec_init is a tensor that represents the state in which the decoder \n",
    "            will start at next stage.\n",
    "            hdec_init is of shape (1, batch_size, hidden_dim)\n",
    "    note:\n",
    "        - in code nb_decoder_timesteps is self.max_decoder_sequence_length\n",
    "        - in code output_dim is self.decoder_vocabulary_size\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(\n",
    "        self, encoder_input, decoder_input=None, teacher_enforce=True, inference=False\n",
    "    ):\n",
    "\n",
    "        batch_size = encoder_input.size()[1]\n",
    "        if inference:\n",
    "            assert (\n",
    "                batch_size == 1\n",
    "            ), \"during inference batch size must be 1: 1 sequence processed\"\n",
    "            if teacher_enforce:\n",
    "                print(\"Warning teacher_enforce will be set to False for inference\")\n",
    "                teacher_enforce = False\n",
    "\n",
    "        # TODO:\n",
    "        henc_ts = None\n",
    "        henc_final = None\n",
    "\n",
    "        if teacher_enforce:\n",
    "            assert decoder_input is not None\n",
    "            # TODO:\n",
    "            pred_softmax_all_ts = torch.zeros(\n",
    "                self.max_decoder_sequence_length,\n",
    "                batch_size,\n",
    "                self.decoder_vocabulary_size,\n",
    "                requires_grad=True,\n",
    "            )\n",
    "            hdec_final = None\n",
    "\n",
    "        elif not teacher_enforce:\n",
    "            pred_softmax_all_ts = []\n",
    "            # TODO:\n",
    "            decoder_input = torch.zeros(1, batch_size, self.decoder_vocabulary_size)\n",
    "\n",
    "            decoder_input = decoder_input.to(self.device)\n",
    "            hdec_init = henc_final\n",
    "            # iterate over all decoder stages\n",
    "            for _ in range(self.max_decoder_sequence_length):\n",
    "                # TODO:\n",
    "                pred_softmax = torch.zeros(\n",
    "                    1, batch_size, self.decoder_vocabulary_size, requires_grad=True\n",
    "                )\n",
    "                hdec_final = None\n",
    "                pred_softmax_all_ts.append(pred_softmax)\n",
    "                # convert softmax predictions to idx\n",
    "                preds_idx = pred_softmax.argmax(dim=2)\n",
    "                # convert idx predictions to one-hot encoding\n",
    "                decoder_input = torch.zeros(1, batch_size, self.decoder_vocabulary_size)\n",
    "                decoder_input = decoder_input.to(self.device)\n",
    "                decoder_input[0, np.arange(batch_size), preds_idx] = 1\n",
    "\n",
    "                # TODO:\n",
    "                hdec_init = None\n",
    "                if inference:\n",
    "                    pred = preds_idx.squeeze().item()\n",
    "                    if pred == self.decoder_char_index[self.EOS]:\n",
    "                        break\n",
    "            pred_softmax_all_ts = torch.cat(pred_softmax_all_ts)\n",
    "\n",
    "        return pred_softmax_all_ts\n",
    "\n",
    "    def _train_on_batch(\n",
    "        self, encoder_input, target, teacher_forcing, decoder_input=None\n",
    "    ):\n",
    "        self.optimizer.zero_grad()\n",
    "        prediction = self.forward(\n",
    "            encoder_input, decoder_input=decoder_input, teacher_enforce=teacher_forcing\n",
    "        )\n",
    "        target_idx = target.argmax(2)\n",
    "        loss_on_batch = self.criterion(\n",
    "            prediction.reshape(-1, prediction.size()[2]), target_idx.reshape(-1)\n",
    "        )\n",
    "        loss_on_batch.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss_on_batch\n",
    "\n",
    "    def train(self, nb_epoch=10, batch_size=64, teacher_enforce=True):\n",
    "        arr = np.arange(self.encoder_input_tr.size()[1])\n",
    "        np.random.shuffle(arr)\n",
    "        nb_batch = int(self.encoder_input_tr.size()[1] / batch_size)\n",
    "        verbose_every = 5 if nb_batch >= 5 else 1\n",
    "\n",
    "        for epoch in range(nb_epoch):\n",
    "            self._reset_monitor_train_epoch()\n",
    "            if epoch > 0:\n",
    "                print()\n",
    "            for batch_idx in range(nb_batch):\n",
    "                idxs = arr[batch_idx * batch_size : (batch_idx + 1) * batch_size]\n",
    "                encoder_input_batch_tr = self.encoder_input_tr[:, idxs, :]\n",
    "                target_batch_tr = self.target_tr[:, idxs, :]\n",
    "                decoder_input_batch_tr = self.decoder_input_tr[:, idxs, :]\n",
    "\n",
    "                batch_loss_tr = self._train_on_batch(\n",
    "                    encoder_input_batch_tr,\n",
    "                    target_batch_tr,\n",
    "                    teacher_forcing=teacher_enforce,\n",
    "                    decoder_input=decoder_input_batch_tr,\n",
    "                )\n",
    "                self._monitor_train_epoch(\n",
    "                    batch_loss=batch_loss_tr,\n",
    "                    batch_size=encoder_input_batch_tr.size()[1],\n",
    "                )\n",
    "\n",
    "                if (batch_idx + 1) % verbose_every == 0:\n",
    "                    self._display_training(\n",
    "                        epoch, nb_epoch, batch_idx, nb_batch, epoch_ended=False\n",
    "                    )\n",
    "\n",
    "            self._monitor_validation(teacher_enforce=teacher_enforce)\n",
    "            self._display_training(\n",
    "                epoch, nb_epoch, batch_idx, nb_batch, epoch_ended=True\n",
    "            )\n",
    "\n",
    "    def _monitor_train_epoch(self, batch_loss, batch_size):\n",
    "        self.total_loss += batch_loss * batch_size\n",
    "        self.total_loss_nb_samples += batch_size\n",
    "\n",
    "    def _reset_monitor_train_epoch(self):\n",
    "        self.total_loss = 0\n",
    "        self.total_loss_nb_samples = 0\n",
    "\n",
    "    def _monitor_validation(self, teacher_enforce):\n",
    "\n",
    "        prediction_val = self(\n",
    "            self.encoder_input_val,\n",
    "            decoder_input=self.decoder_input_val,\n",
    "            teacher_enforce=teacher_enforce,\n",
    "        )\n",
    "        target_val_idx = self.target_val.argmax(2)\n",
    "        self.last_loss_val = self.criterion(\n",
    "            prediction_val.reshape(-1, prediction_val.size()[2]),\n",
    "            target_val_idx.reshape(-1),\n",
    "        )\n",
    "\n",
    "    def _display_training(\n",
    "        self, epoch, nb_epoch, idx_batch, nb_batch, epoch_ended=False\n",
    "    ):\n",
    "        msg = \"Epoch {}/{} {} {}\".format(\n",
    "            epoch + 1,\n",
    "            nb_epoch,\n",
    "            utils.arrow(idx_batch + 1, nb_batch),\n",
    "            \" mean loss: %.5f\" % (self.total_loss.item() / self.total_loss_nb_samples),\n",
    "        )\n",
    "        if epoch_ended:\n",
    "            msg += \" val loss: %.5f\" % self.last_loss_val\n",
    "        print(msg, end=\"\\r\")\n",
    "\n",
    "    def _tensor_to_words(self, output, decoded=True):\n",
    "        dict_index_char = (\n",
    "            self.decoder_index_char if decoded else self.encoder_index_char\n",
    "        )\n",
    "        pred_idx = output.argmax(dim=2)\n",
    "        decoded_words = []\n",
    "        for seq in range(pred_idx.size()[1]):\n",
    "            idxs_chars = pred_idx[:, seq]\n",
    "            decoded_word = \"\".join(dict_index_char[idx.item()] for idx in idxs_chars)\n",
    "            if not decoded:\n",
    "                # correct errors due to zero vectors at the end\n",
    "                accepted_end_chars = set(list(\"0123456789\"))\n",
    "                for i in range(len(decoded_word) - 1, -1, -1):\n",
    "                    if decoded_word[i] in accepted_end_chars:\n",
    "                        decoded_word = decoded_word[: i + 1]\n",
    "                        break\n",
    "            decoded_words.append(decoded_word)\n",
    "        return decoded_words\n",
    "\n",
    "    def evaluate(self, nb=30):\n",
    "        nb = min(nb, self.encoder_input_val.size()[1])\n",
    "        for i in range(nb):\n",
    "            output = self(\n",
    "                self.encoder_input_val[:, i : i + 1, :],\n",
    "                inference=True,\n",
    "                teacher_enforce=False,\n",
    "            )\n",
    "            decoded_word = self._tensor_to_words(output, decoded=True)[0]\n",
    "            operation = self._tensor_to_words(\n",
    "                self.encoder_input_val[:, i : i + 1, :], decoded=False\n",
    "            )[0][::-1]\n",
    "            expected_decoded_word = self._tensor_to_words(\n",
    "                self.target_val[:, i : i + 1, :], decoded=True\n",
    "            )[0]\n",
    "            decoded_word = decoded_word.replace(\"\\n\", \"\")\n",
    "            operation = operation.replace(\"\\n\", \"\")\n",
    "            expected_decoded_word = expected_decoded_word.replace(\"\\n\", \"\")\n",
    "            print(\n",
    "                \"Input sentence: {} Decoded sentence: {} Expected decoded sentence: {}\".format(\n",
    "                    operation, decoded_word, expected_decoded_word\n",
    "                )\n",
    "            )\n",
    "            print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### no attention - teacher forcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 100\n",
      "Number of unique encoder tokens: 15\n",
      "Number of unique decoder tokens: 14\n",
      "Max sequence length for encoding: 7\n",
      "Max sequence length for decoding: 10\n"
     ]
    }
   ],
   "source": [
    "seq2seq = Seq2seq(X, y, hidden_size=128, attention=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 ==================================================>  mean loss: 0.00000\r",
      "Epoch 1/10 ==================================================>  mean loss: 0.00000 val loss: 0.00000\r\n",
      "Epoch 2/10 ==================================================>  mean loss: 0.00000\r",
      "Epoch 2/10 ==================================================>  mean loss: 0.00000 val loss: 0.00000\r\n",
      "Epoch 3/10 ==================================================>  mean loss: 0.00000\r",
      "Epoch 3/10 ==================================================>  mean loss: 0.00000 val loss: 0.00000\r\n",
      "Epoch 4/10 ==================================================>  mean loss: 0.00000\r",
      "Epoch 4/10 ==================================================>  mean loss: 0.00000 val loss: 0.00000\r\n",
      "Epoch 5/10 ==================================================>  mean loss: 0.00000\r",
      "Epoch 5/10 ==================================================>  mean loss: 0.00000 val loss: 0.00000\r\n",
      "Epoch 6/10 ==================================================>  mean loss: 0.00000\r",
      "Epoch 6/10 ==================================================>  mean loss: 0.00000 val loss: 0.00000\r\n",
      "Epoch 7/10 ==================================================>  mean loss: 0.00000\r",
      "Epoch 7/10 ==================================================>  mean loss: 0.00000 val loss: 0.00000\r\n",
      "Epoch 8/10 ==================================================>  mean loss: 0.00000\r",
      "Epoch 8/10 ==================================================>  mean loss: 0.00000 val loss: 0.00000\r\n",
      "Epoch 9/10 ==================================================>  mean loss: 0.00000\r",
      "Epoch 9/10 ==================================================>  mean loss: 0.00000 val loss: 0.00000\r\n",
      "Epoch 10/10 ==================================================>  mean loss: 0.00000\r",
      "Epoch 10/10 ==================================================>  mean loss: 0.00000 val loss: 0.00000\r"
     ]
    }
   ],
   "source": [
    "seq2seq.train(nb_epoch=10, batch_size=64, teacher_enforce=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sentence: 379%966 Decoded sentence: ========== Expected decoded sentence: 379\n",
      "\n",
      "Input sentence: 902-214 Decoded sentence: ========== Expected decoded sentence: 688\n",
      "\n",
      "Input sentence: 761/121 Decoded sentence: ========== Expected decoded sentence: 6.28926\n",
      "\n",
      "Input sentence: 962-83 Decoded sentence: ========== Expected decoded sentence: 879\n",
      "\n",
      "Input sentence: 753%134 Decoded sentence: ========== Expected decoded sentence: 83\n",
      "\n",
      "Input sentence: 178/347 Decoded sentence: ========== Expected decoded sentence: 0.51297\n",
      "\n",
      "Input sentence: 669/583 Decoded sentence: ========== Expected decoded sentence: 1.14751\n",
      "\n",
      "Input sentence: 937*521 Decoded sentence: ========== Expected decoded sentence: 488177\n",
      "\n",
      "Input sentence: 178-757 Decoded sentence: ========== Expected decoded sentence: -579\n",
      "\n",
      "Input sentence: 124*640 Decoded sentence: ========== Expected decoded sentence: 79360\n",
      "\n",
      "Input sentence: 995-623 Decoded sentence: ========== Expected decoded sentence: 372\n",
      "\n",
      "Input sentence: 882-358 Decoded sentence: ========== Expected decoded sentence: 524\n",
      "\n",
      "Input sentence: 803*776 Decoded sentence: ========== Expected decoded sentence: 623128\n",
      "\n",
      "Input sentence: 985*759 Decoded sentence: ========== Expected decoded sentence: 747615\n",
      "\n",
      "Input sentence: 152%922 Decoded sentence: ========== Expected decoded sentence: 152\n",
      "\n",
      "Input sentence: 506/315 Decoded sentence: ========== Expected decoded sentence: 1.60635\n",
      "\n",
      "Input sentence: 161%485 Decoded sentence: ========== Expected decoded sentence: 161\n",
      "\n",
      "Input sentence: 274/91 Decoded sentence: ========== Expected decoded sentence: 3.01099\n",
      "\n",
      "Input sentence: 969*780 Decoded sentence: ========== Expected decoded sentence: 755820\n",
      "\n",
      "Input sentence: 193*906 Decoded sentence: ========== Expected decoded sentence: 174858\n",
      "\n",
      "Input sentence: 138+206 Decoded sentence: ========== Expected decoded sentence: 344\n",
      "\n",
      "Input sentence: 672-737 Decoded sentence: ========== Expected decoded sentence: -65\n",
      "\n",
      "Input sentence: 232-26 Decoded sentence: ========== Expected decoded sentence: 206\n",
      "\n",
      "Input sentence: 973+357 Decoded sentence: ========== Expected decoded sentence: 1330\n",
      "\n",
      "Input sentence: 587+595 Decoded sentence: ========== Expected decoded sentence: 1182\n",
      "\n"
     ]
    }
   ],
   "source": [
    "seq2seq.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### no attention - no teacher forcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq = Seq2seq(X, y, hidden_size=128, attention=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq.train(nb_epoch=3, batch_size=64, teacher_enforce=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### attention - teacher forcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq_attn = Seq2seq(X, y, hidden_size=128, attention=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq_attn.train(nb_epoch=3, batch_size=64, teacher_enforce=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq_attn.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### attention - no teacher forcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq_attn = Seq2seq(X, y, hidden_size=128, attention=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq_attn.train(nb_epoch=3, batch_size=64, teacher_enforce=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq_attn.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###¬†Questions:\n",
    "- 1) Explain the interest in using teacher forcing during training. What is specific about this process?\n",
    "\n",
    "\n",
    "\n",
    "- 2) Describe step by step how the encoder-decoder couple works in this case (~ 5-10 lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###¬†Questions:\n",
    "- 1) Describe how the attention mechanism works in the seq2seq setting (~ 5-10 lines)\n",
    "\n",
    "Attention mechanism works by being able to focus on a specific subsequence in a long sequence to predict the right token at some timestep. That means not having to rely solely on final  ‚Ñéùëíùëõùëêùëá  to predict the whole decoded sequence, but rather recombining and weighing all the  ‚Ñéùëíùëõùëêùë°  at each decoding step to focus those related to the prediction. At each decoding step, a scalar product is performed between  ‚Ñéùëëùëíùëêùë°  and all the  ‚Ñéùëíùëõùëêùë° . This gives a similarity measure between  ‚Ñéùëëùëíùëêùë°  and each  ‚Ñéùëíùëõùëêùë° . A softmax is applied to this vector to rescale the similarity coefficients and make them sum to  1 . This way we can use them to compute a mean  ‚Ñéùëíùëõùëê  vector to be used for prediction that allows the network to focus on some input tokens by making some coefficient relatively much greater than the others. Mean  ‚Ñéùëíùëõùëê  vector is then computed and followed by  ùë°ùëéùëõ‚Ñé  operation to reduce vector input space of next operation. Final step is a softmax fully connected layer over the  ùë°ùëéùëõ‚Ñé  vector for prediction of the next decoded token. Applying attention mechanism involves iterating over this for each decoding timestep.\n",
    "\n",
    "The attention mechanism recombine all the weights at each step in the decoding. Thanks to that, the model is able to have better prediction on long expressions. It will not only focus on the final $h^{enc}$. \n",
    "To implement the attention mechanism, we do a scalar product between $h^{dec}_t$ and ALL the $h^{enc}_t$. Then we apply a softmax function to this result. Then we can compute a mean vector we can use to predict the next token. \n",
    "The inconvenient of such a mechanism is that we have to go through all theses steps at each timestep. \n",
    "\n",
    "- 2) Compare the perfomances of your model at inference time with and without attention mechanism. Do you see noticeable differences? Why?\n",
    "\n",
    "Here, we are not hoping for a huge difference in performances between the model with and without the attention mechanism. Almost all the \"sentences\" here are not huge and moreover, almost all tokens in each expression is involved in the computation of the result. Thus it is not relevant to use the attention mechanism here, as it would have been in a case of translation for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
